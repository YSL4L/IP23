{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1ff77ba",
   "metadata": {},
   "source": [
    "# Translating with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d003e13",
   "metadata": {},
   "source": [
    "NLTK (Natural Language Toolkit) is a suite of libraries and programs for natural language processing.  NLTK provides tools for tokenization, part-of-speech tagging, stemming, chunking, parsing, and semantic reasoning, among other tasks. It also includes a large collection of corpora, lexicons etc. \n",
    "For this Task relevant is the nltk.corpus package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235ba39b",
   "metadata": {},
   "source": [
    "\n",
    "First, we need to download NLTK and the datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfecac4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_13284/1191373820.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\krusic\\AppData\\Local\\Temp/ipykernel_13284/1191373820.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    pip install nltk\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1379035f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krusic\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ca986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# This Code will download all the data from nltk and open a window in which you can see the download progress. \n",
    "nltk.download() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdeb9e2",
   "metadata": {},
   "source": [
    "# Translating a file with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02476d3c",
   "metadata": {},
   "source": [
    "This code imports the NLTK library and the comtrans module from NLTK, and loads the bilingual English-french corpus. It then creates a translation dictionary using the words and mots from the corpus. It then opens an English file, tokenizes it, and translates it using the translation dictionary. Finally, it writes the German (in this case french) version of the document to a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef8b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import comtrans\n",
    "\n",
    "# load the english-french corpus\n",
    "bilingual_corpus = comtrans.aligned_sents('alignment-en-fr.txt')\n",
    "\n",
    "# create a translation dictionary\n",
    "trans_dict = dict()\n",
    "for sent in english_german:\n",
    "    for english, german in zip(sent.words, sent.mots):\n",
    "        # add the english-german translation to the dictionary\n",
    "        trans_dict[english] = german\n",
    "\n",
    "# open and read the english file\n",
    "with open(\"ice_man.txt\", encoding='utf-8') as f:\n",
    "    english_file = f.read()\n",
    "\n",
    "# tokenize the english file\n",
    "english_tokens = nltk.word_tokenize(english_file)\n",
    "\n",
    "# translate the english file\n",
    "german_file = \"\"\n",
    "for token in english_tokens:\n",
    "    if token in trans_dict:\n",
    "        german_file += trans_dict[token] + \" \"\n",
    "    else:\n",
    "        german_file += token + \" \"\n",
    "\n",
    "# write the german file\n",
    "with open(\"german_file.txt\", \"w\", encoding='utf-8') as f:\n",
    "    f.write(german_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485cf9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this, we can only translate from english to french"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1582e8e",
   "metadata": {},
   "source": [
    "# Python \"translate\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b4b80a",
   "metadata": {},
   "source": [
    "An other option to translate is the python \"translate' libary. For this we need to install and import translate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8a73a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install translate \n",
    "import translate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da5c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#again we need to read the file\n",
    "with open('ice_man.txt', 'r', encoding='utf-8') as f: \n",
    "    text = f.read() \n",
    "    \n",
    "#with this code we translate from english to german\n",
    "translator = translate.Translator(from_lang=\"english\",to_lang=\"german\")\n",
    "translation = translator.translate(text) \n",
    "\n",
    "#now we create a new file called \"output.txt\" with the german translation in it.\n",
    "with open('output.txt', 'w', encoding='utf-8') as f: \n",
    "    f.write(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e297e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!#With this option we can only translate texts that have less than 500 chars!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844c2c9e",
   "metadata": {},
   "source": [
    "# Googletrans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcac9db6",
   "metadata": {},
   "source": [
    "We can also use googletrans to translate in python (max. 15.000 chars) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46f6051",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install googletrans==3.1.0a0\n",
    "from googletrans import Translator\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d84cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ice_man.txt', 'r', encoding='utf-8') as f: \n",
    "    text = f.read()\n",
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "translation = translator.translate(text,dest='de') \n",
    "\n",
    "print(translation.text)\n",
    "with open ('google.txt','w') as f:\n",
    "    f.write(translation.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca88ca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I get a AttributeError: 'NoneType' object has no attribute 'group'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9a06c4",
   "metadata": {},
   "source": [
    "# Calculating the sentiment with Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31992074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code imports the TextBlob module from textblob library to perform sentiment analysis. \n",
    "from textblob import TextBlob \n",
    "\n",
    "#reads the file and assigns the content to the variable 'text_en'.\n",
    "with open(\"ice_man.txt\") as f: \n",
    "    text_en = f.read()\n",
    "blob_en = TextBlob(text_en) #creates a TextBlob object with text_en. \n",
    "\n",
    "print(\"Sentiment of English File:\") #prints out the sentiment of the file.\n",
    "print(blob_en.sentiment)\n",
    "\n",
    "#reads the german_file and assigns the content to the variable 'text_de'\n",
    "with open(\"german_file.txt\") as f: \n",
    "    text_de = f.read()\n",
    "blob_de = TextBlob(text_de) #creates a TextBlob object with text_de\n",
    "\n",
    "print(\"Sentiment of German File:\") \n",
    "print(blob_de.sentiment)\n",
    "if blob_en.sentiment != blob_de.sentiment: #checks if the sentiment of the English and German files are the same.\n",
    "    print(\"The sentiment of the English and German files differ.\")\n",
    "else:\n",
    "    print(\"The sentiment of the English and German files are the same.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ed706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc19af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
