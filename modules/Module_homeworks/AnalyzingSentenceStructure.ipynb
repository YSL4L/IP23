{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ba2a4a",
   "metadata": {},
   "source": [
    "# Analyzing Sentence Structure\n",
    "\n",
    "Natural Language ToolKit (NLTK) is a comprehensive Python library for natural language processing and text analytics.\n",
    "The chapter \"Analyzing Sentence Structure\" in general deals with the ambiguity that natural language is famous for and tries to find ways to cope with the fact that there are an unlimited number of possible sentences, and that there can only be written finite programs to analyze their structures and discover their meanings. \n",
    "\n",
    "The chapter deals with answering the following questions: \n",
    "\n",
    "    How can we use a formal grammar to describe the structure of an unlimited set of sentences?\n",
    "    How do we represent the structure of sentences using syntax trees?\n",
    "    How do parsers analyze a sentence and automatically build a syntax tree?\n",
    "\n",
    "\n",
    "In this notebook, we will take a practical approach to these topics. Our tasks are:       \n",
    "\n",
    "    - read the txt file\n",
    "    - use nltk tokenizer to divide sentences into words, so that each sentence is a list e.g. sentence = ['I', 'am', 'cold]\n",
    "    - use  pos parsing  to assign tags (verb, noun, adverb) to the words in the list and creates the parsing trees\n",
    "    - save (write) the sentence trees \n",
    "\n",
    "So first, the file \"ice_man.txt\" is opened and read.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c7e168",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ice_man.txt', 'r', encoding='utf-8') as file:\n",
    "    data = file.read().rstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c273a77",
   "metadata": {},
   "source": [
    "After that, the nltk library is downloaded with the following command: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecead827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8f7940",
   "metadata": {},
   "source": [
    "The first task is to use the nltk tokenizer to divide sentences into words, so that each sentence is a list. \n",
    "\n",
    "## Tokenization \n",
    "FIRST: get rid of punctuation.\n",
    "A process by which a large quantity of text is divided into smaller parts called tokens. These tokens are very useful for finding patterns and are considered as a base step for stemming and lemmatization. Tokenization also helps to substitute sensitive data elements with non-sensitive data elements.\n",
    "\n",
    "To tokenize the sample text, it has to be opened (and read) first. The whole text has to be opened as a string, to be able to process it further. \n",
    "\"Sent_tokenize\" splits the whole text into sentences. \"word_tokenize()\" splits the whole text into tokens. \"[word_tokenize(word) for word in sent_tokenize(data)]\" divides the text into sentences and splits these sentences into tokens. You can try out all 3 commands below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c541eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a960d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430fe469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "pos = [word_tokenize(word) for word in sent_tokenize(data)]\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c890a57",
   "metadata": {},
   "source": [
    "The output is a list, containing each sentence as a list. \n",
    "The next task is to\tuse pos parsing to assign tags (verb, noun, adverb etc.) to the words in the list and to create the corresponding parsing trees.\n",
    "\n",
    "To do so, we tokenize the data again. This time, we use a version of the text, where the punctuation is removed. In the processes before punctuation was needed, to be able to use the \"sent_tokenize()\" command. \n",
    "Then, the module \"pos_tag\" and \"word_tokenize\" have to be imported. Eventually, pos parsing is implemented with \"pos_tag(tokenizer)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e11ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "strdata = tokenizer.tokenize(data)\n",
    "\n",
    "#the data is transformed to a string again, to be able to use tokenize it in the next step\n",
    "data2 = ' '.join(strdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c6af2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "tokenizer = word_tokenize(data2)\n",
    "\n",
    "# Find all parts of speech in the text\n",
    "textpos = pos_tag(tokenizer)\n",
    "textpos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89befa81",
   "metadata": {},
   "source": [
    "This is one option to tokenize and assign tags to each word. Another possible way to do so is shown below. In this case, the grammar, on the basis of which the pos-tagging is performed, is defined manually. *Abbreviations: DT=Determiner, JJ=Adjective, NN=Noun, IN/p=Preposition, V=Verb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60178fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag, word_tokenize, RegexpParser\n",
    "\n",
    "\n",
    "# Find all parts of speech in above sentence\n",
    "tagged = pos_tag(word_tokenize(data2))\n",
    "\n",
    "#define grammer: \n",
    "grammar= (\"\"\"\n",
    "                    \n",
    "                    NP: {<DT>?<JJ>*<NN>} #To extract Noun Phrases\n",
    "                    P: {<IN>}            #To extract Prepositions\n",
    "                    V: {<V.*>}           #To extract Verbs\n",
    "                    PP: {<p> <NP>}       #To extract Prepositional Phrases\n",
    "                    VP: {<V> <NP|PP>*}   #To extract Verb Phrases\n",
    "                    \"\"\")\n",
    "\n",
    "#Extract all parts of speech from any text\n",
    "chunker = RegexpParser(grammar)\n",
    "\n",
    "# Print all parts of speech in above sentence\n",
    "output = chunker.parse(tagged)\n",
    "#print('Filename:output.ps', output, file=output)\n",
    "#listo =set(output)\n",
    "#listo.print_to_file('output.ps')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856ac90c",
   "metadata": {},
   "source": [
    "The next step is to output the graphic representation of the sentence trees.  \n",
    "To be able to take a look at the graphic representations of the actual syntax trees for each sentence, you can apply the following function. Due to the fact, that it outputs a syntax tree for each sentence it may take some time. \n",
    "To accomplish this task, TextBlob is used. TextBlob is a Python (2 and 3) library for processing textual data. With it it is possible to dive into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\n",
    "\n",
    "First, TextBlob is imported. After that, the txt file is opened and an (empty) array is created. \n",
    "Part-of-speech tags can be accessed through the tags property. Use the parse() method to parse the text.\n",
    "For example, in the code below it is stated, that an NP chunk should be formed whenever the chunker finds an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN), In = Präpositionen, VDB Verb Phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b8355f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#textblob\n",
    "from textblob import TextBlob\n",
    "with open('ice_man.txt', 'rU') as ins:\n",
    "    array = []\n",
    "    for line in ins:\n",
    "        array.append(line)\n",
    "for i in array:\n",
    "    wiki = TextBlob(i)\n",
    "    a=wiki.tags\n",
    "    sentence = a\n",
    "    pattern =  \"\"\"NP: {<DT>?<JJ>*<NN>}   \n",
    "                    P: {<IN>}            \n",
    "                    V: {<V.*>}           \n",
    "                    PP: {<p> <NP>}       \n",
    "                    VP: {<V> <NP|PP>*}   \n",
    "                    \"\"\"\n",
    "    \n",
    "    #\"\"\"NP: {<DT>?<JJ>*<NN>}\n",
    "    #VBD: {<VBD>}\n",
    "    #IN: {<IN>}\"\"\"\n",
    "    NPChunker = nltk.RegexpParser(pattern)\n",
    "    result = NPChunker.parse(sentence)\n",
    "\n",
    "    result.draw()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6544b917",
   "metadata": {},
   "source": [
    "The sentence trees can also be created with a nltk module. This version is shown below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f01059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tree import Tree\n",
    "from nltk.draw.tree import TreeView\n",
    "\n",
    "t = Tree.draw(textpos)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949f02ab",
   "metadata": {},
   "source": [
    "Note: I wasn´t able to save the sentence trees, due to the error 'Tree' object has no attribute 'write'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7648ba22",
   "metadata": {},
   "source": [
    "## Sources: \n",
    "\n",
    "    - https://www.nltk.org/book/ch08.html\n",
    "    - https://textblob.readthedocs.io/en/dev/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbfe16c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c9662e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
